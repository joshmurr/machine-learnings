{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40326c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "48a98fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6cac469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 28938 characters, 69 unique.\n"
     ]
    }
   ],
   "source": [
    "text = open('rainer-maria-rilke.txt', 'r').read() # Reads as chars\n",
    "block_size = 128\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e2a636ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size):\n",
    "        assert n_embed % n_head == 0\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, n_embed)\n",
    "        self.query = nn.Linear(n_embed, n_embed)\n",
    "        self.value = nn.Linear(n_embed, n_embed)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(0.1)\n",
    "        self.resid_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        \n",
    "        self.mask = torch.tril(torch.ones(block_size, block_size)) \\\n",
    "                    .view(1, 1, block_size, block_size).cuda()\n",
    "            \n",
    "        self.n_head = n_head\n",
    "        \n",
    "        print(f\"Block n_head: {n_head}\")\n",
    "        \n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.attn = SelfAttention(n_embed, n_head, block_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(0.1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class picoGPT(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, n_layers, block_size, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embed))\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        \n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head, block_size) \\\n",
    "                                      for _ in range(n_layers)])\n",
    "        \n",
    "        # decoder\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        print(f\"number of parameters: {sum(p.numel() for p in self.parameters())}\")\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        batch, token = x.size()\n",
    "        assert token <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "        \n",
    "        token_embeddings = self.tok_emb(x)\n",
    "        position_embeddings = self.pos_emb[:, :token, :]\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "79fbcd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block n_head: 2\n",
      "Block n_head: 2\n",
      "number of parameters: 33984\n"
     ]
    }
   ],
   "source": [
    "n_embed = 32\n",
    "n_head = 2\n",
    "n_layers = 2\n",
    "\n",
    "model = picoGPT(\n",
    "    n_embed,\n",
    "    n_head,\n",
    "    n_layers,\n",
    "    train_dataset.block_size,\n",
    "    train_dataset.vocab_size\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4bc58927",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=6e-4, \n",
    "    betas=(0.9, 0.95)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a4e32c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    loader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        batch_size=1,\n",
    "                        num_workers=4\n",
    "                       )\n",
    "    losses = []\n",
    "    \n",
    "    for it, (x, y) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits, loss = model(x, y)\n",
    "            loss = loss.mean()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9eab420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:42<00:00, 129.31it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:44<00:00, 128.50it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:43<00:00, 128.94it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:40<00:00, 130.58it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:23<00:00, 141.77it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:41<00:00, 129.87it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:43<00:00, 129.14it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:20<00:00, 143.64it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:40<00:00, 130.91it/s]\n",
      "100%|███████████████████████████████████████████████████| 28810/28810 [03:40<00:00, 130.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    run_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "09af551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "steps = 110\n",
    "\n",
    "x = \"Then I am shaken\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in x], dtype=torch.long)[None,...].cuda()\n",
    "do_sample = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size]\n",
    "        logits, _ = model(x_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        ix = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        x = torch.cat((x, ix), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d3c9121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then I am shaken a soft\n",
      "Whosu ast I as thopthere floats on.\n",
      "\n",
      "\n",
      "A with caddes with ras fuore became in chese—\n",
      "Ando the dark, tha\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for c in [train_dataset.itos[int(i)] for i in x[0]]:\n",
    "    if c == '\\n':\n",
    "        print(result)\n",
    "        result = \"\"\n",
    "        continue\n",
    "    result += c\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab3596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
