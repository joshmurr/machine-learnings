{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-Keras-Cutsom-Dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3OiuWUeJYEfGU1MY769dF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshmurr/machine-learnings/blob/master/cci-dsai/DCGAN_Keras_Cutsom_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1CXqrESDzz5"
      },
      "source": [
        "# DCGAN - Custom Dataset\n",
        "## Deep Convolutional GAN\n",
        "\n",
        "This is much the same as the previous notebook we looked at (still using the [official Keras DCGAN implementation](https://keras.io/examples/generative/dcgan_overriding_train_step/)) but I have some extra code at the start to allow you to make your own dataset from a YouTube video!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcXXG7NMcHAl"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "import cv2\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CotjyJ1-ewMz",
        "outputId": "bd07b271-58a5-4097-b2bf-8d037408224d"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFQoTc66Mi4f"
      },
      "source": [
        "We have to install [YouTube-DL](https://youtube-dl.org/) via pip in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMkxqCTQif06",
        "outputId": "2e1f13af-ea95-4ea6-b552-ce35c5f36884"
      },
      "source": [
        "!pip install --upgrade --quiet youtube_dl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9MB 8.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSx3LqLKMveN"
      },
      "source": [
        "All you need to do is to run the cell below. It just contains helper functions to download a YouTube video and to extract frames from the video to make the dataset. Feel free to poke around if you're interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCo8qlByilMl"
      },
      "source": [
        "# Some helper functions we will use to make the dataset\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "import youtube_dl\n",
        "\n",
        "\n",
        "class MyLogger(object):\n",
        "    def debug(self, msg):\n",
        "        pass\n",
        "\n",
        "    def warning(self, msg):\n",
        "        pass\n",
        "\n",
        "    def error(self, msg):\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "def my_hook(d):\n",
        "    if d['status'] == 'finished':\n",
        "        print('Done downloading.')\n",
        "\n",
        "def download_youtube_video(_url):\n",
        "  ydl_opts = {\n",
        "      'format': '(mp4)[height>=256][height<=400]',\n",
        "      'outtmpl': '%(id)s.%(ext)s',\n",
        "      'logger': MyLogger(),\n",
        "      'progress_hooks': [my_hook],\n",
        "  }\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "      result = ydl.extract_info(_url, download=True)\n",
        "\n",
        "  if 'entries' in result:\n",
        "    video = result['entries'][0]\n",
        "  else:\n",
        "    video = result\n",
        "\n",
        "  return video\n",
        "\n",
        "def analyse_video(_videoPath):\n",
        "  vidcap = cv2.VideoCapture(_videoPath)\n",
        "  success, frame = vidcap.read()\n",
        "\n",
        "  frameCount = 0\n",
        "  darkFrames = []\n",
        "  validFrames = []\n",
        "\n",
        "  while success:\n",
        "    grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    average = grey.mean(axis=0).mean(axis=0)\n",
        "\n",
        "    if average < 2:\n",
        "      darkFrames.append(frameCount)\n",
        "    else:\n",
        "      validFrames.append(frameCount)\n",
        "    \n",
        "    success, frame = vidcap.read()\n",
        "    frameCount += 1\n",
        "  \n",
        "  print(f'Found {len(darkFrames)} dark frames.')\n",
        "  return validFrames, darkFrames\n",
        "\n",
        "def extract_frames(_videoPath, _outputPath, _num, _size):\n",
        "  SIZE = _size[0]\n",
        "  MAX = _num\n",
        "  count = 0\n",
        "  id = 0\n",
        "  validFrames, darkFrames = analyse_video(_videoPath)\n",
        "  doubles = []\n",
        "  frames = []\n",
        "\n",
        "  if MAX > len(validFrames):\n",
        "    numDoubles = MAX - len(validFrames)\n",
        "    doubles = np.random.choice(validFrames, size=numDoubles, replace=False)\n",
        "    frames = validFrames\n",
        "  else:\n",
        "    frames = np.random.choice(validFrames, size=MAX, replace=False)\n",
        "\n",
        "  vidcap = cv2.VideoCapture(_videoPath)\n",
        "  success, frame = vidcap.read()\n",
        "\n",
        "  frameHeight = frame.shape[0]\n",
        "  frameWidth = frame.shape[1]\n",
        "\n",
        "  scaleFactor = SIZE / frameHeight\n",
        "  newWidth = int(frameWidth * scaleFactor)\n",
        "  padding = int((newWidth - SIZE) / 2)\n",
        "\n",
        "  while success:\n",
        "    if count in frames:\n",
        "      frame = cv2.resize(frame, (newWidth, SIZE), interpolation=cv2.INTER_AREA)\n",
        "      crops = []\n",
        "\n",
        "      if count in doubles:\n",
        "        crops = [frame[0:SIZE, 0:SIZE],\n",
        "                frame[0:SIZE, padding*2:SIZE+padding*2]]\n",
        "      else:\n",
        "        crops = [frame[0:SIZE, padding:SIZE+padding]]\n",
        "\n",
        "      for crop in crops:\n",
        "        try:\n",
        "          cv2.imwrite(os.path.join(_outputPath, f'{id:04}.jpg'), crop)\n",
        "          id += 1\n",
        "        except:\n",
        "          print(\"Error saving frame.\")\n",
        "          pass\n",
        "    \n",
        "    count += 1\n",
        "    success, frame = vidcap.read()\n",
        "\n",
        "  print(f\"Saved {id} images from video '{videoInfo['title']}'\")\n",
        "\n",
        "  return id"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvkuYrrKMZ8"
      },
      "source": [
        "# Choose a YouTube Video\n",
        "\n",
        "Find a video on YouTube which is about 4-10 minutes long. The video can be anything but ideally it will be generally consistent throughout. So timelapse video are perfect, [like this video of clouds forming](https://www.youtube.com/watch?v=NJfI_GaEyJw), or [this video of life underwater](https://www.youtube.com/watch?v=J2BKd5e15Jc) has nice consistent colours and forms. However it is entirely up to you, maybe it would be interesting to get a random video of Lady Gaga dresses.. who knows!\n",
        "\n",
        "Paste the YouTube video URL in the cell below, __replacing the url that is between the single quotes__.\n",
        "\n",
        "If the URL is long like this:\n",
        "\n",
        "```\n",
        "https://www.youtube.com/watch?v=NJfI_GaEyJw&ab_channel=wizard327\n",
        "                                           ^\n",
        "                       We don't need the stuff after this & symbol.\n",
        "```\n",
        "\n",
        "just trim the end off after (and including) the '__&__' symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoIgi2xYisel",
        "outputId": "9fc5af7c-212e-44a0-b3f2-6bb6344fe5ac"
      },
      "source": [
        "url = 'https://www.youtube.com/watch?v=J2BKd5e15Jc' # Coral Reef\n",
        "videoInfo = download_youtube_video(url)\n",
        "videoFile = \"{0}.{1}\".format(videoInfo['webpage_url'].split('=')[-1], videoInfo['ext'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done downloading.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_-OFQLWL-U8"
      },
      "source": [
        "If your video has an intro and an outro we can trim that off! Change the `startTime` and `endTime` values below. The values need to be in seconds, so if the good bit of the video start at 38 seconds then enter `38` for `startTime`. If the end credits start at 9:14, then an easy way to find the seconds is `(9*60)+14`.\n",
        "\n",
        "__If you don't need to trim the video down then just don't run the cell below.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m61KiSm-iugQ",
        "outputId": "66aa725a-bb23-4344-b659-0e57c0b01f22"
      },
      "source": [
        "# Optional trimming of the video to remove intro / end credits.\n",
        "# Skip this if your video does not need trimming.\n",
        "\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "\n",
        "def trim_video(_video, _start, _end):\n",
        "  trimmedVideo = f\"trimmed.{videoInfo['ext']}\"\n",
        "  ffmpeg_extract_subclip(videoFile, _start, _end, targetname=trimmedVideo)\n",
        "  return trimmedVideo\n",
        "\n",
        "\n",
        "startTime = 38\n",
        "endTime = (9*60)+14\n",
        "videoFile = trim_video(videoFile, startTime, endTime)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[MoviePy] Running:\n",
            ">>> /usr/bin/ffmpeg -y -i J2BKd5e15Jc.mp4 -ss 38.00 -t 516.00 -vcodec copy -acodec copy trimmed.mp4\n",
            "... command successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWdv0dpWN7Gh"
      },
      "source": [
        "Next we make some directories and extract 3000 images from the the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRHTno-9cMG4",
        "outputId": "968dd77a-e0ca-458c-a7ca-17e3fd9e16cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DATASET_DIR = 'dataset'\n",
        "IMAGES_DIR = f\"{DATASET_DIR}/images\"\n",
        "OUTPUT_DIR = 'output'\n",
        "IMAGE_SIZE =(64, 64)\n",
        "try:\n",
        "  os.makedirs(DATASET_DIR)\n",
        "  os.makedirs(IMAGES_DIR)\n",
        "  os.makedirs(OUTPUT_DIR)\n",
        "except:\n",
        "  pass\n",
        "NUM_IMAGES = extract_frames(f\"/content/{videoFile}\", IMAGES_DIR, 3000, IMAGE_SIZE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 31 dark frames.\n",
            "Saved 3000 images from video 'Exploring the Coral Reef: Learn about Oceans for Kids - FreeSchool'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcVHKvapOCSa"
      },
      "source": [
        "As with before we then turn it into a _Tensorflow dataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEpCzNfCc0_7",
        "outputId": "66b4d2e5-341f-4275-f907-78bc965f866d"
      },
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    DATASET_DIR, label_mode=None, image_size=IMAGE_SIZE, batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3000 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28RcFXw8OGsY"
      },
      "source": [
        "All things going well, if you run the cell below you should see frames from the video you chose being taken from the dataset we created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "DNOYzTQac1od",
        "outputId": "7a3b3aa0-cbe3-4ad9-d71b-ccaf8b07efbd"
      },
      "source": [
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahot01TYOOKW"
      },
      "source": [
        "# Defining the Model\n",
        "\n",
        "This is the same as the previous example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPjWwh8CdeK8",
        "outputId": "e2945b29-1792-4736-e2d4-bd470414cb2b"
      },
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 8193      \n",
            "=================================================================\n",
            "Total params: 404,801\n",
            "Trainable params: 404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve5WyO5vdhg7",
        "outputId": "50b2c151-9fa4-4667-c048-0242f1eaa00a"
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 8192)              1056768   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 256)       524544    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 3)         38403     \n",
            "=================================================================\n",
            "Total params: 3,979,651\n",
            "Trainable params: 3,979,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28BVDjCNdnT4"
      },
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlygRVfXdrCK"
      },
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=1, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save(os.path.join(OUTPUT_DIR, \"generated_img_%03d_%d.png\" % (epoch, i)))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilBtgRC2OS6E"
      },
      "source": [
        "# Train the GAN\n",
        "\n",
        "Now our dataset is much much smaller than [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) and so this will train a lot quicker.\n",
        "\n",
        "Unfortunately Keras likes to output a lot of it's own information during training which makes it difficult to display images using `imshow` or something similar. But as with before images from the generator are saved to a directory called `output` which you can find in the file explorer on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjCwDPALdtgA",
        "outputId": "8fcf594e-5ed4-46c9-a4f6-e06b0649157a"
      },
      "source": [
        "epochs = 60\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=1, latent_dim=latent_dim)]\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "94/94 [==============================] - 22s 151ms/step - d_loss: 0.5275 - g_loss: 0.9996\n",
            "Epoch 2/60\n",
            "94/94 [==============================] - 14s 146ms/step - d_loss: 0.5887 - g_loss: 1.7964\n",
            "Epoch 3/60\n",
            "94/94 [==============================] - 14s 146ms/step - d_loss: 0.5530 - g_loss: 1.4173\n",
            "Epoch 4/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5841 - g_loss: 1.5787\n",
            "Epoch 5/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.4916 - g_loss: 1.2724\n",
            "Epoch 6/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.4265 - g_loss: 1.6158\n",
            "Epoch 7/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.4993 - g_loss: 1.3737\n",
            "Epoch 8/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6907 - g_loss: 1.1980\n",
            "Epoch 9/60\n",
            "94/94 [==============================] - 14s 146ms/step - d_loss: 0.5081 - g_loss: 1.5337\n",
            "Epoch 10/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5974 - g_loss: 1.5541\n",
            "Epoch 11/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5772 - g_loss: 1.2183\n",
            "Epoch 12/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6124 - g_loss: 1.0926\n",
            "Epoch 13/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6370 - g_loss: 1.2903\n",
            "Epoch 14/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5708 - g_loss: 1.1313\n",
            "Epoch 15/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6055 - g_loss: 1.8010\n",
            "Epoch 16/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6301 - g_loss: 1.2536\n",
            "Epoch 17/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6271 - g_loss: 1.0749\n",
            "Epoch 18/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.4901 - g_loss: 1.4472\n",
            "Epoch 19/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6697 - g_loss: 1.1394\n",
            "Epoch 20/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5836 - g_loss: 1.3269\n",
            "Epoch 21/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5603 - g_loss: 1.1876\n",
            "Epoch 22/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6735 - g_loss: 0.8300\n",
            "Epoch 23/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.5949 - g_loss: 0.9437\n",
            "Epoch 24/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6718 - g_loss: 0.9716\n",
            "Epoch 25/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6293 - g_loss: 1.0172\n",
            "Epoch 26/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6071 - g_loss: 1.2407\n",
            "Epoch 27/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6070 - g_loss: 1.5164\n",
            "Epoch 28/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.7105 - g_loss: 1.1202\n",
            "Epoch 29/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6199 - g_loss: 1.0459\n",
            "Epoch 30/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6290 - g_loss: 0.9600\n",
            "Epoch 31/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6089 - g_loss: 1.0733\n",
            "Epoch 32/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6497 - g_loss: 0.9275\n",
            "Epoch 33/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6722 - g_loss: 0.9549\n",
            "Epoch 34/60\n",
            "94/94 [==============================] - 14s 149ms/step - d_loss: 0.5913 - g_loss: 1.0827\n",
            "Epoch 35/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6026 - g_loss: 1.2522\n",
            "Epoch 36/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6472 - g_loss: 1.0060\n",
            "Epoch 37/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6311 - g_loss: 0.9663\n",
            "Epoch 38/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6501 - g_loss: 1.0237\n",
            "Epoch 39/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6537 - g_loss: 0.9115\n",
            "Epoch 40/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6569 - g_loss: 0.9443\n",
            "Epoch 41/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6239 - g_loss: 1.0425\n",
            "Epoch 42/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6538 - g_loss: 0.9330\n",
            "Epoch 43/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6647 - g_loss: 1.1493\n",
            "Epoch 44/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6558 - g_loss: 1.2626\n",
            "Epoch 45/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5930 - g_loss: 1.6771\n",
            "Epoch 46/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6259 - g_loss: 0.9905\n",
            "Epoch 47/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6497 - g_loss: 0.9062\n",
            "Epoch 48/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6854 - g_loss: 0.9346\n",
            "Epoch 49/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6287 - g_loss: 0.9586\n",
            "Epoch 50/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.6512 - g_loss: 0.9798\n",
            "Epoch 51/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6431 - g_loss: 0.9683\n",
            "Epoch 52/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6675 - g_loss: 0.8618\n",
            "Epoch 53/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6706 - g_loss: 0.9857\n",
            "Epoch 54/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6635 - g_loss: 1.1198\n",
            "Epoch 55/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.7113 - g_loss: 1.2452\n",
            "Epoch 56/60\n",
            "94/94 [==============================] - 14s 149ms/step - d_loss: 0.6489 - g_loss: 1.2294\n",
            "Epoch 57/60\n",
            "94/94 [==============================] - 14s 147ms/step - d_loss: 0.5941 - g_loss: 1.0952\n",
            "Epoch 58/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.7320 - g_loss: 1.0535\n",
            "Epoch 59/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6077 - g_loss: 1.2434\n",
            "Epoch 60/60\n",
            "94/94 [==============================] - 14s 148ms/step - d_loss: 0.6734 - g_loss: 0.8577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4f17a9f310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0tPBlhsI5vm"
      },
      "source": [
        "# Extra: Saving a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFwq3AyHiAW",
        "outputId": "15dc259e-6866-4f07-d4b3-285d900b6172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Make a new directory to save the model into\n",
        "os.mkdir('model')\n",
        "gan.generator.save('model') # We're just saving the generator here as that's the interesting bit!"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTQ8KHFIHzG5",
        "outputId": "97ae7706-e74d-4cd5-fba9-146c8eadd3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Give Colab access to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9tFTa7OH8oj"
      },
      "source": [
        "# Copy the 'model' folder to your drive\n",
        "# -r means 'copy recursively' so it copies all the files and folders\n",
        "# inside the 'model' directory.\n",
        "!cp -r /content/model/ /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}